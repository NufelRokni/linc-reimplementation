"""Inference utilities for FOLIO logical reasoning.

This module encapsulates the logic required to generate predictions
from a language model given a formatted prompt.  It implements both
single generation and majority voting over multiple samples.  Two
parsing functions are provided: one for the baseline mode which
extracts the first occurrence of a label anywhere in the generation
and another for the scratchpad mode which preferentially looks for a
label immediately following an ``ANSWER:`` tag.  The ``infer_once_or_vote``
function dispatches to the appropriate parser based on the value of
``args.mode``.
"""

from __future__ import annotations

import re
from collections import Counter
from typing import Optional, Tuple

__all__ = [
    "parse_label_from_text",
    "parse_label_from_scratchpad",
    "generate_once",
    "infer_once_or_vote",
]

# Regular expression used to extract one of the expected labels from a
# generation.  It is deliberately case insensitive and anchored to
# whole word boundaries to avoid accidental matches within longer
# strings.
_LABEL_PATTERN = re.compile(r"\b(True|False|Uncertain)\b", re.IGNORECASE)

# Regular expression to capture a label following an ANSWER tag.
_ANSWER_PATTERN = re.compile(r"ANSWER:\s*(True|False|Uncertain)\b", re.IGNORECASE)


def parse_label_from_text(text: str) -> Optional[str]:
    """Extract the first occurrence of a canonical label from generated text.

    :param text: Raw string generated by the language model.
    :returns: The canonical label if found, otherwise ``None``.
    """
    m = _LABEL_PATTERN.search(text)
    if not m:
        return None
    return m.group(1).capitalize()


def parse_label_from_scratchpad(text: str) -> Optional[str]:
    """Extract a label from a scratchpad generation.

    Scratchpad prompts include an ``ANSWER:`` tag.  This parser looks
    preferentially for a label immediately following that tag.  If
    absent, it falls back to searching anywhere in the text for one of
    the canonical labels.

    :param text: Raw string generated by the language model.
    :returns: The canonical label if found, otherwise ``None``.
    """
    m = _ANSWER_PATTERN.search(text)
    if m:
        return m.group(1).capitalize()
    # Fallback to generic pattern
    return parse_label_from_text(text)


def generate_once(model, tokenizer, prompt: str, args) -> str:
    """Generate a continuation from the model given a prompt.

    This function wraps the underlying ``model.generate`` call.  It
    delegates decisions about sampling versus greedy decoding to the
    caller via attributes of ``args`` (namely ``temperature`` and
    ``top_p``).  Generation is performed without gradients and only
    the continuation tokens are returned (the prompt itself is
    stripped off).

    :param model: A HuggingFace CausalLM model.
    :param tokenizer: The corresponding tokenizer.
    :param prompt: The input prompt string.
    :param args: Parsed arguments with generation parameters.
    :returns: The generated continuation as a stripped string.
    """
    # Import torch lazily here to avoid a hard dependency at module import time
    import torch  # type: ignore

    model_inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    do_sample = bool(args.temperature and args.temperature > 0)
    gen_kwargs = dict(
        max_new_tokens=args.max_new_tokens,
        do_sample=do_sample,
        temperature=float(args.temperature) if do_sample else None,
        top_p=float(args.top_p) if do_sample else None,
        pad_token_id=tokenizer.eos_token_id,
        eos_token_id=tokenizer.eos_token_id,
    )
    # Remove any None values from kwargs to avoid warnings
    gen_kwargs = {k: v for k, v in gen_kwargs.items() if v is not None}
    with torch.no_grad():
        out = model.generate(**model_inputs, **gen_kwargs)
    # slice only the continuation tokens (safer than naive string slicing)
    prompt_len = model_inputs["input_ids"].shape[-1]
    continuation_ids = out[0][prompt_len:]
    gen_text = tokenizer.decode(continuation_ids, skip_special_tokens=True)
    return gen_text.strip()


def infer_once_or_vote(model, tokenizer, prompt: str, args) -> Tuple[str, Counter]:
    """Generate one or more samples and return the majority label.

    When ``args.n_samples`` is greater than one the function repeatedly
    calls ``generate_once`` and tallies votes for each predicted label.
    Unparsable generations are counted as ``Uncertain`` by default.  The
    most frequent label (ties broken arbitrarily) and the vote counts
    are returned.

    :param model: HuggingFace CausalLM model.
    :param tokenizer: Corresponding tokenizer.
    :param prompt: Input prompt string.
    :param args: Parsed arguments with ``n_samples``, ``verbose`` and ``mode`` fields.
    :returns: Tuple of (predicted label, Counter of votes).
    """
    votes: Counter = Counter()
    mode = getattr(args, "mode", "baseline")
    for _ in range(max(1, args.n_samples)):
        gen = generate_once(model, tokenizer, prompt, args)
        if mode == "scratchpad":
            lab = parse_label_from_scratchpad(gen) or "Uncertain"
        else:
            lab = parse_label_from_text(gen) or "Uncertain"
        votes[lab] += 1
        if getattr(args, "verbose", False):
            print(f"  sample -> {gen!r} => {lab}")
    pred: str = votes.most_common(1)[0][0]
    return pred, votes